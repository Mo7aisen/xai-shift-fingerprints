#!/usr/bin/env python3
"""Auto-generate Table 2 rows from NIH effect size CSV + hypothesis tests.

Outputs:
  - manuscript/tables/table2_autogenerated.tex
  - final_elsevier_submission/tables/table2_autogenerated.tex
  - submission_medical_image_analysis/tables/table2_autogenerated.tex
"""

from __future__ import annotations

from pathlib import Path

import pandas as pd


def _format_effect(value: float) -> str:
    if pd.isna(value):
        return "NA"
    return f"{value:.3f}"


def _format_ci(low: float, high: float) -> str:
    if pd.isna(low) or pd.isna(high):
        return "NA"
    return f"[{low:.3f}, {high:.3f}]"


def _format_p(value: float) -> str:
    if pd.isna(value):
        return "NA"
    # Numerical underflow can produce exact zeros for extremely small p-values.
    if value <= 0:
        return "$<10^{-300}$"
    if value < 1e-4:
        return f"{value:.2e}"
    return f"{value:.3f}"


def main() -> None:
    root = Path(__file__).resolve().parents[1]
    effects_path = root / "reports" / "enhanced_statistics" / "nih_effect_sizes.csv"
    pvals_path = root / "results" / "metrics" / "divergence" / "hypothesis_tests.csv"
    output_paths = [
        root / "manuscript" / "tables" / "table2_autogenerated.tex",
        root / "final_elsevier_submission" / "tables" / "table2_autogenerated.tex",
        root / "submission_medical_image_analysis" / "tables" / "table2_autogenerated.tex",
    ]
    for output_path in output_paths:
        output_path.parent.mkdir(parents=True, exist_ok=True)

    effects_df = pd.read_csv(effects_path)
    pvals_df = pd.read_csv(pvals_path)

    metric_labels = {
        "dice": "Dice",
        "coverage_auc": "Coverage AUC",
        "attribution_abs_sum": "Attr Mass",
        "border_abs_sum": "Border Mass",
        "hist_entropy": "Entropy",
    }
    metric_order = [
        "dice",
        "coverage_auc",
        "attribution_abs_sum",
        "border_abs_sum",
        "hist_entropy",
    ]

    # Normalize comparison labels in p-value table
    pvals_df = pvals_df[pvals_df["comparison"].str.contains("NIH", na=False)].copy()
    pvals_df["comparison_short"] = pvals_df["comparison"].str.replace(" Baseline", "", regex=False)

    comparisons = ["JSRT vs NIH", "Montgomery vs NIH", "Shenzhen vs NIH"]

    rows = []
    for comp in comparisons:
        for metric in metric_order:
            eff_row = effects_df[(effects_df["comparison"] == comp) & (effects_df["metric"] == metric)]
            p_row = pvals_df[(pvals_df["comparison_short"] == comp) & (pvals_df["metric"] == metric)]

            cliffs = float(eff_row["cliffs_delta"].iloc[0]) if not eff_row.empty else float("nan")
            ci_low = float(eff_row["cliffs_ci_low"].iloc[0]) if not eff_row.empty else float("nan")
            ci_high = float(eff_row["cliffs_ci_high"].iloc[0]) if not eff_row.empty else float("nan")

            perm_fdr = float(p_row["permutation_p_fdr"].iloc[0]) if not p_row.empty else float("nan")
            t_fdr = float(p_row["t_test_p_fdr"].iloc[0]) if not p_row.empty else float("nan")

            row = (
                f"{comp} & {metric_labels[metric]} & "
                f"{_format_effect(cliffs)} & {_format_ci(ci_low, ci_high)} & "
                f"{_format_p(perm_fdr)} & {_format_p(t_fdr)} \\\\"
            )
            rows.append(row)

    rendered = "\n".join(rows) + "\n"
    for output_path in output_paths:
        output_path.write_text(rendered, encoding="utf-8")
        print(f"[INFO] Wrote {output_path}")


if __name__ == "__main__":
    main()
